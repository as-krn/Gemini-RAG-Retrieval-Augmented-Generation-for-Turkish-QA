from duckduckgo_search import DDGS
from sentence_transformers import SentenceTransformer, CrossEncoder
import faiss
import numpy as np
import os
import re
import nltk
from nltk.tokenize import sent_tokenize
from typing import List, Tuple, Dict
import time
import math
import logging
import scipy.special
import google.generativeai as genai

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Google Gemini ayarlarƒ±
GOOGLE_API_KEY = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'
genai.configure(api_key=GOOGLE_API_KEY)

def upload_to_gemini(path, mime_type=None):
    """Gemini'ye dosya y√ºkleme fonksiyonu"""
    try:
        file = genai.upload_file(path, mime_type=mime_type)
        print(f"Dosya y√ºklendi: '{file.display_name}' as: {file.uri}")
        return file
    except Exception as e:
        print(f"Dosya y√ºkleme hatasƒ±: {str(e)}")
        return None

def wait_for_files_active(files):
    """Dosya i≈üleme tamamlanana kadar bekle"""
    print("Dosya i≈üleniyor...")
    for file in files:
        if file is None:
            continue
        retry_count = 0
        max_retries = 20
        while file.state.name == "PROCESSING" and retry_count < max_retries:
            print(".", end="", flush=True)
            time.sleep(3)
            try:
                file = genai.get_file(file.name)
                retry_count += 1
            except Exception as e:
                print(f"Dosya durumu kontrol hatasƒ±: {e}")
                break
        if file.state.name != "ACTIVE":
            print(f"Dosya {file.name} i≈ülenemedi, durum: {file.state.name}")
        else:
            print(f"Dosya {file.name} hazƒ±r")
    print("\nT√ºm dosyalar hazƒ±r")

# Geli≈ümi≈ü sistem talimatlarƒ±
system_instruction = """
Sen T√ºrk√ße konu≈üan bir AI asistanƒ±sƒ±n. Kullanƒ±cƒ±nƒ±n sorularƒ±nƒ± verilen baƒülam ve belgeler doƒürultusunda yanƒ±tla.

KURALLARIN:
1. Sadece verilen baƒülam ve belgelerden yararlan
2. Eƒüer baƒülamda yeterli bilgi yoksa bunu a√ßƒ±k√ßa belirt
3. Yanƒ±tlarƒ±nƒ± kƒ±sa, net ve anla≈üƒ±lƒ±r tut
4. T√ºrk√ße dilbilgisi kurallarƒ±na uy
5. Kaynak belirtmeyi unutma
6. Spek√ºlasyon yapma, sadece belgelerden elde ettiƒüin bilgileri kullan

YANIT FORMATI:
- Doƒürudan cevap ver
- Gerekirse madde madde listele
- Kaynaklarƒ± belirt
- Belirsizlik varsa bunu s√∂yle
"""

generation_config = {
    "temperature": 0.3,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 1024,
    "response_mime_type": "text/plain",
}

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    generation_config=generation_config,
    system_instruction=system_instruction,
)

class ImprovedRAGPipeline:
    def __init__(self, docs_path="data/documents.txt"):
        self.docs_path = docs_path
        self.embedding_model = SentenceTransformer("emrecan/bert-base-turkish-cased-mean-nli-stsb-tr")
        self.re_ranker = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2")
        self.documents = []
        self.document_metadata = []
        self.index = None
        self.doc_embeddings = None
        
        # Logging kurulumu
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def load_documents(self) -> bool:
        """Belgeleri y√ºkle ve i≈üle"""
        if not os.path.exists(self.docs_path):
            self.logger.warning(f"Dosya bulunamadƒ±: {self.docs_path}")
            return False
        
        try:
            with open(self.docs_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
            
            self.documents, self.document_metadata = [], []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # URL ayrƒ±≈ütƒ±rma
                if " ||| " in line:
                    content, url = line.split(" ||| ", 1)
                    url = url.strip("() ")
                else:
                    content = line
                    url = "Bilinmeyen kaynak"
                
                # Metin temizleme
                cleaned = self._clean_text(content)
                if len(cleaned) > 50:
                    # Metin par√ßalara ayƒ±rma
                    chunks = self._split_into_chunks(cleaned, max_sentences=5)
                    for chunk in chunks:
                        self.documents.append(chunk)
                        self.document_metadata.append({
                            "url": url, 
                            "length": len(chunk),
                            "original_content": content[:200] + "..." if len(content) > 200 else content
                        })
            
            self.logger.info(f"{len(self.documents)} belge par√ßasƒ± y√ºklendi.")
            return True
            
        except Exception as e:
            self.logger.error(f"Belge y√ºkleme hatasƒ±: {str(e)}")
            return False

    def _split_into_chunks(self, text: str, max_sentences: int = 5) -> List[str]:
        """Metni c√ºmle bazƒ±nda par√ßalara ayƒ±r"""
        try:
            sentences = sent_tokenize(text, language='turkish')
            chunks = []
            for i in range(0, len(sentences), max_sentences):
                chunk = " ".join(sentences[i:i+max_sentences])
                if len(chunk.strip()) > 30:  # √áok kƒ±sa par√ßalarƒ± filtrele
                    chunks.append(chunk)
            return chunks
        except Exception as e:
            self.logger.error(f"Metin par√ßalama hatasƒ±: {str(e)}")
            return [text]

    def _clean_text(self, text: str) -> str:
        """Metni temizle"""
        # HTML etiketlerini kaldƒ±r
        text = re.sub(r'<[^>]+>', '', text)
        # Fazla bo≈üluklarƒ± d√ºzelt
        text = re.sub(r'\s+', ' ', text)
        # Sadece istenen karakterleri tut
        text = re.sub(r'[^\w\s.,!?;:()\-√ºƒüƒ±√∂√ß≈ü√úƒûI√ñ√á≈û]', '', text)
        return text.strip()

    def build_faiss_index(self) -> bool:
        """FAISS indeksi olu≈ütur"""
        if not self.documents:
            self.logger.warning("Y√ºklenecek dok√ºman yok.")
            return False
        
        try:
            self.logger.info("Embedding'ler hesaplanƒ±yor...")
            embeddings = self.embedding_model.encode(
                self.documents, 
                convert_to_numpy=True, 
                show_progress_bar=True, 
                batch_size=32
            )
            
            # Normalize embeddings
            faiss.normalize_L2(embeddings)
            
            # FAISS indeks olu≈ütur
            self.index = faiss.IndexFlatIP(embeddings.shape[1])
            self.index.add(embeddings)
            self.doc_embeddings = embeddings
            
            self.logger.info("FAISS indeksi ba≈üarƒ±yla olu≈üturuldu.")
            return True
            
        except Exception as e:
            self.logger.error(f"ƒ∞ndeks olu≈üturma hatasƒ±: {str(e)}")
            return False

    def expand_query(self, query: str) -> str:
        """Sorguyu geni≈ület"""
        replacements = {
            "yapay zeka": "yapay zeka artificial intelligence AI",
            "makine √∂ƒürenmesi": "makine √∂ƒürenmesi machine learning ML",
            "veri bilimi": "veri bilimi data science",
            "derin √∂ƒürenme": "derin √∂ƒürenme deep learning",
            "algoritma": "algoritma algorithm",
            "model": "model algoritma sistem"
        }
        
        query_lower = query.lower()
        for key, val in replacements.items():
            if key in query_lower:
                query = query.replace(key, val)
        
        return query

    def retrieve(self, query: str, top_k: int = 10) -> List[Tuple[str, float, str]]:
        """ƒ∞lgili belgeleri getir"""
        if self.index is None:
            self.logger.warning("FAISS indeksi yok.")
            return []
        
        try:
            # Sorguyu geni≈ület
            expanded_query = self.expand_query(query)
            
            # Query embedding'ini hesapla
            query_vec = self.embedding_model.encode([expanded_query], convert_to_numpy=True)
            faiss.normalize_L2(query_vec)
            
            # Arama yap
            similarities, indices = self.index.search(query_vec, top_k)
            
            # Sonu√ßlarƒ± d√ºzenle
            docs = []
            for sim, idx in zip(similarities[0], indices[0]):
                if idx >= 0 and sim > 0.1:  # Minimum benzerlik e≈üiƒüi
                    doc_content = self.documents[idx]
                    doc_url = self.document_metadata[idx]["url"]
                    docs.append((doc_content, float(sim), doc_url))
            
            # Re-ranking uygula
            if docs:
                try:
                    rerank_pairs = [(query, doc) for doc, _, _ in docs]
                    rerank_scores = self.re_ranker.predict(rerank_pairs)
                    
                    # Skorlarƒ± g√ºncelleyerek yeniden sƒ±rala
                    docs = [(doc, float(score), url) for (doc, _, url), score in zip(docs, rerank_scores)]
                    docs.sort(key=lambda x: x[1], reverse=True)
                    
                except Exception as e:
                    self.logger.warning(f"Re-ranking hatasƒ±: {str(e)}")
            
            return docs
            
        except Exception as e:
            self.logger.error(f"Retrieval hatasƒ±: {str(e)}")
            return []

class OptimizedGeminiRAG(ImprovedRAGPipeline):
    def __init__(self, docs_path="data/documents.txt", pdf_path=None):
        super().__init__(docs_path)
        self.pdf_path = pdf_path
        self.gemini_file = None
        self.chat_session = None
        self.conversation_history = []
        self.api_call_count = 0
        self.max_api_calls_per_minute = 60

    def prepare_gemini(self):
        """Gemini'yi hazƒ±rla"""
        try:
            history = []
            
            # PDF varsa y√ºkle
            if self.pdf_path and os.path.exists(self.pdf_path):
                self.logger.info(f"PDF y√ºkleniyor: {self.pdf_path}")
                self.gemini_file = upload_to_gemini(self.pdf_path, mime_type="application/pdf")
                if self.gemini_file:
                    wait_for_files_active([self.gemini_file])
                    history.append({
                        "role": "user",
                        "parts": [self.gemini_file],
                    })
            
            # Chat session ba≈ülat
            self.chat_session = model.start_chat(history=history)
            self.logger.info("Gemini hazƒ±rlandƒ±.")
            
        except Exception as e:
            self.logger.error(f"Gemini hazƒ±rlama hatasƒ±: {str(e)}")
            raise

    def _build_context(self, retrieved_docs: List[Tuple[str, float, str]], max_context_length: int = 3000) -> str:
        """Retrieved belgelerden context olu≈ütur"""
        context_parts = []
        total_length = 0
        
        for i, (doc, score, url) in enumerate(retrieved_docs):
            if total_length + len(doc) > max_context_length:
                break
            
            context_parts.append(f"BELGE {i+1} (Kaynak: {url}):\n{doc}\n")
            total_length += len(doc)
        
        return "\n".join(context_parts)

    def _create_enhanced_prompt(self, query: str, context: str) -> str:
        """Geli≈ümi≈ü prompt olu≈ütur"""
        prompt = f"""
BAƒûLAM Bƒ∞LGƒ∞LERƒ∞:
{context}

KULLANICI SORUSU: {query}

G√ñREV:
Yukarƒ±daki baƒülam bilgilerini kullanarak kullanƒ±cƒ±nƒ±n sorusunu yanƒ±tla.

KURALLARIN:
1. Sadece verilen baƒülam bilgilerini kullan
2. Baƒülamda yeterli bilgi yoksa bunu a√ßƒ±k√ßa belirt
3. Yanƒ±tƒ±nƒ± kƒ±sa ve √∂z tut (maksimum 3-4 paragraf)
4. Hangi belgeden hangi bilgiyi aldƒ±ƒüƒ±nƒ± belirt
5. Emin olmadƒ±ƒüƒ±n konularda spek√ºlasyon yapma

YANIT:
"""
        return prompt

    def _rate_limit_check(self):
        """API rate limit kontrol√º"""
        if self.api_call_count >= self.max_api_calls_per_minute:
            self.logger.info("Rate limit reached, waiting...")
            time.sleep(60)
            self.api_call_count = 0

    def _fallback_response(self, query: str) -> Dict[str, any]:
        """Fallback yanƒ±t"""
        return {
            "answer": "√úzg√ºn√ºm, bu soruya yanƒ±t veremiyorum. L√ºtfen daha sonra tekrar deneyin.",
            "sources": [],
            "confidence": 0.0,
            "retrieved_count": 0,
            "method": "fallback"
        }

    def _validate_response(self, response_text: str, retrieved_docs: List[Tuple[str, float, str]]) -> float:
        """Yanƒ±t kalitesini deƒüerlendir"""
        if not response_text or len(response_text.strip()) < 20:
            return 0.1
        
        # Kaynak belgelerdeki kelimelerin yanƒ±tta ge√ßme oranƒ±
        if not retrieved_docs:
            return 0.5
        
        try:
            doc_words = set()
            for doc, _, _ in retrieved_docs:
                doc_words.update(doc.lower().split())
            
            response_words = set(response_text.lower().split())
            overlap = len(doc_words.intersection(response_words))
            
            if len(doc_words) > 0:
                relevance_score = min(overlap / len(doc_words), 1.0)
                return max(relevance_score, 0.3)
            
        except Exception as e:
            self.logger.warning(f"Yanƒ±t validation hatasƒ±: {str(e)}")
        
        return 0.5

    def generate_answer(self, query: str, use_retrieval: bool = True) -> Dict[str, any]:
        """Gemini kullanarak yanƒ±t √ºret"""
        if self.chat_session is None:
            try:
                self.prepare_gemini()
            except Exception as e:
                self.logger.error(f"Gemini hazƒ±rlanamadƒ±: {str(e)}")
                return self._fallback_response(query)

        try:
            # Rate limit kontrol√º
            self._rate_limit_check()
            
            retrieved_docs = []
            context = ""
            
            # Belge retrieval kullan
            if use_retrieval:
                retrieved_docs = self.retrieve(query, top_k=5)
                if retrieved_docs:
                    context = self._build_context(retrieved_docs)
                    self.logger.info(f"{len(retrieved_docs)} belge getirildi.")
                else:
                    self.logger.warning("Relevant belge bulunamadƒ±.")
            
            # Prompt olu≈ütur
            if context:
                prompt = self._create_enhanced_prompt(query, context)
            else:
                prompt = f"""
KULLANICI SORUSU: {query}

Eƒüer bu soruyla ilgili bilginiz varsa yanƒ±tlayƒ±n. Yoksa "Bu konuda yeterli bilgim yok" deyin.
Yanƒ±tƒ±nƒ±zƒ± kƒ±sa ve √∂z tutun.
"""

            # Gemini'ye g√∂nder
            self.api_call_count += 1
            response = self.chat_session.send_message(prompt)
            answer_text = response.text.strip()
            
            # Yanƒ±t validation
            confidence = self._validate_response(answer_text, retrieved_docs)
            
            # Yanƒ±t yapƒ±landƒ±r
            sources = []
            for doc, score, url in retrieved_docs[:3]:  # ƒ∞lk 3 kaynaƒüƒ± g√∂ster
                sources.append({
                    "text": doc[:200] + "..." if len(doc) > 200 else doc,
                    "url": url,
                    "similarity": round(score, 3)
                })
            
            result = {
                "answer": answer_text if answer_text.endswith('.') else answer_text + ".",
                "sources": sources,
                "confidence": confidence,
                "retrieved_count": len(retrieved_docs),
                "method": "gemini_with_retrieval" if use_retrieval else "gemini_only"
            }
            
            # Conversation history g√ºncelle
            self.conversation_history.append({
                "query": query,
                "response": answer_text,
                "timestamp": time.time()
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"Gemini yanƒ±t √ºretme hatasƒ±: {str(e)}")
            return self._fallback_response(query)

    def get_conversation_summary(self) -> str:
        """Konu≈üma √∂zetini al"""
        if not self.conversation_history:
            return "Hen√ºz konu≈üma ge√ßmi≈üi yok."
        
        summary = f"Toplam {len(self.conversation_history)} soru-cevap:\n\n"
        for i, conv in enumerate(self.conversation_history[-5:], 1):  # Son 5 konu≈üma
            summary += f"{i}. S: {conv['query'][:100]}...\n"
            summary += f"   C: {conv['response'][:100]}...\n\n"
        
        return summary

def search_web_improved(query: str, num_results: int = 10, save_path: str = "data/documents.txt") -> List[Tuple[str, str]]:
    """Geli≈ümi≈ü web arama"""
    results = []
    max_retries = 3
    retry_wait = 5

    print(f"Web aramasƒ±: '{query}'")

    for attempt in range(1, max_retries + 1):
        try:
            with DDGS() as ddgs:
                search_results = ddgs.text(query, max_results=num_results)
                
                for r in search_results:
                    snippet = r.get('body', '').replace('\n', ' ').strip()
                    url = r.get('href', '')
                    title = r.get('title', '')

                    full_content = f"{title}. {snippet}" if title else snippet

                    if len(full_content) > 100:
                        results.append((full_content, url))

            if results:
                # Sonu√ßlarƒ± kaydet
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                with open(save_path, "w", encoding="utf-8") as f:
                    for content, url in results:
                        f.write(f"{content} ||| {url}\n")
                print(f"{len(results)} sonu√ß kaydedildi.")
                return results

        except Exception as e:
            print(f"[Deneme {attempt}] Hata: {str(e)}")
            if "Ratelimit" in str(e) and attempt < max_retries:
                print(f"{retry_wait} saniye bekleniyor...")
                time.sleep(retry_wait)
                retry_wait *= 2  # Exponential backoff
            else:
                break

    print("Web aramasƒ± ba≈üarƒ±sƒ±z.")
    return []

def main():
    """Ana fonksiyon"""
    # RAG sistemini ba≈ülat
    rag = OptimizedGeminiRAG(docs_path="data/documents.txt")
    
    # Test sorgularƒ±
    test_queries = [
        "Yapay zeka nedir ve nasƒ±l √ßalƒ±≈üƒ±r?",
        "Makine √∂ƒürenmesi algoritmalarƒ± nelerdir?",
        "Derin √∂ƒürenme ile geleneksel makine √∂ƒürenmesi arasƒ±ndaki fark nedir?",
        "Doƒüal dil i≈üleme nedir?"
    ]
    
    print("=== GELƒ∞≈ûMƒ∞≈û GEMƒ∞Nƒ∞ RAG Sƒ∞STEMƒ∞ ===\n")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*60}")
        print(f"TEST {i}: {query}")
        print('='*60)
        
        # Web aramasƒ± yap
        search_results = search_web_improved(query, num_results=8)
        
        if search_results:
            # Belgeleri y√ºkle ve indeks olu≈ütur
            if rag.load_documents() and rag.build_faiss_index():
                # Yanƒ±t √ºret
                response = rag.generate_answer(query)
                
                print(f"\nüìù YANIT:")
                print(f"{response['answer']}\n")
                
                print(f"üìä Bƒ∞LGƒ∞LER:")
                print(f"‚Ä¢ G√ºven skoru: {response['confidence']:.2f}")
                print(f"‚Ä¢ Bulunan belge: {response['retrieved_count']}")
                print(f"‚Ä¢ Y√∂ntem: {response['method']}")
                
                if response['sources']:
                    print(f"\nüìö KAYNAKLAR:")
                    for j, source in enumerate(response['sources'], 1):
                        print(f"{j}. {source['url']} (Benzerlik: {source['similarity']})")
                        print(f"   {source['text'][:100]}...")
                
            else:
                print("‚ùå Belge y√ºkleme veya indeks olu≈üturma hatasƒ±")
        else:
            print("‚ùå Web aramasƒ± ba≈üarƒ±sƒ±z")
        
        print(f"\n{'='*60}")
        time.sleep(2)  # Rate limiting i√ßin bekle
    
    # Konu≈üma √∂zetini g√∂ster
    print(f"\nüìã KONU≈ûMA √ñZETƒ∞:")
    print(rag.get_conversation_summary())

if __name__ == "__main__":
    main()